import tensorflow as tf
import numpy as np

def load_data():
    print("تحميل البيانات من ملف ara_eng.txt...")
    file_path = '/kaggle/input/dataset/ara_eng.txt'
    
    en_sentences = []
    ar_sentences = []
    
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            if line.strip():
                try:
                    en, ar = line.strip().split('\t')
                    en_sentences.append(en)
                    ar_sentences.append(ar)
                except ValueError:
                    print(f"تم تجاهل سطر غير منسق: {line.strip()}")
                    continue
    
    print(f"تم تحميل {len(en_sentences)} جملة.")
    return en_sentences, ar_sentences

# تنفيذ الجزء
en_sentences, ar_sentences = load_data()

print("أول جملة إنجليزية:", en_sentences[0])
print("أول جملة عربية:", ar_sentences[0])


import tensorflow as tf
import unicodedata
import re

def preprocess_sentence(sentence, is_arabic=False):
    sentence = sentence.lower().strip()
    
    if is_arabic:
        sentence = re.sub(r'[\u0617-\u061A\u064B-\u0652]', '', sentence)
        sentence = re.sub(r'[^\w\s\u0600-\u06FF]', '', sentence)
    else:
        sentence = re.sub(r'[^\w\s]', '', sentence)
    
    return '<start> ' + sentence + ' <end>'

def prepare_data(en_sentences, ar_sentences, max_length=50):
    en_processed = [preprocess_sentence(s) for s in en_sentences if s.strip()]
    ar_processed = [preprocess_sentence(s, is_arabic=True) for s in ar_sentences if s.strip()]
    
    en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')
    ar_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')
    
    en_tokenizer.fit_on_texts(en_processed)
    ar_tokenizer.fit_on_texts(ar_processed)
    
    en_sequences = en_tokenizer.texts_to_sequences(en_processed)
    ar_sequences = ar_tokenizer.texts_to_sequences(ar_processed)
    
    # Padding
    en_padded = tf.keras.preprocessing.sequence.pad_sequences(en_sequences, maxlen=max_length, padding='post')
    ar_padded = tf.keras.preprocessing.sequence.pad_sequences(ar_sequences, maxlen=max_length, padding='post')
    
    print(f"حجم المفردات الإنجليزية: {len(en_tokenizer.word_index)}")
    print(f"حجم المفردات العربية: {len(ar_tokenizer.word_index)}")
    return en_padded, ar_padded, en_tokenizer, ar_tokenizer

max_length = 50
en_padded, ar_padded, en_tokenizer, ar_tokenizer = prepare_data(en_sentences, ar_sentences, max_length)

print("أول تسلسل إنجليزي:", en_padded[0])
print("أول تسلسل عربي:", ar_padded[0])


import tensorflow as tf

def create_padding_mask(seq):
    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)
    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)

def create_look_ahead_mask(size):
    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
    return mask  # (seq_len, seq_len)

class MultiHeadAttention(tf.keras.layers.Layer):
    def _init_(self, d_model, num_heads):
        super(MultiHeadAttention, self)._init_()
        self.num_heads = num_heads
        self.d_model = d_model
        assert d_model % num_heads == 0
        self.depth = d_model // num_heads
        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)
        self.dense = tf.keras.layers.Dense(d_model)

    def split_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, v, k, q, mask=None):
        batch_size = tf.shape(q)[0]
        q = self.wq(q)
        k = self.wk(k)
        v = self.wv(v)
        q = self.split_heads(q, batch_size)
        k = self.split_heads(k, batch_size)
        v = self.split_heads(v, batch_size)

        scaled_attention = scaled_dot_product_attention(q, k, v, mask)
        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))
        output = self.dense(concat_attention)
        return output

def scaled_dot_product_attention(q, k, v, mask=None):
    matmul_qk = tf.matmul(q, k, transpose_b=True)
    dk = tf.cast(tf.shape(k)[-1], tf.float32)
    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)
    if mask is not None:
        scaled_attention_logits += (mask * -1e9)
    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
    output = tf.matmul(attention_weights, v)
    return output

class EncoderLayer(tf.keras.layers.Layer):
    def _init_(self, d_model, num_heads, dff):
        super(EncoderLayer, self)._init_()
        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(dff, activation='relu'),
            tf.keras.layers.Dense(d_model)
        ])
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = tf.keras.layers.Dropout(0.1)
        self.dropout2 = tf.keras.layers.Dropout(0.1)

    def call(self, x, training=False, mask=None):
        attn_output = self.mha(x, x, x, mask)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)
        return out2

class DecoderLayer(tf.keras.layers.Layer):
    def _init_(self, d_model, num_heads, dff):
        super(DecoderLayer, self)._init_()
        self.mha1 = MultiHeadAttention(d_model, num_heads)
        self.mha2 = MultiHeadAttention(d_model, num_heads)
        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(dff, activation='relu'),
            tf.keras.layers.Dense(d_model)
        ])
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = tf.keras.layers.Dropout(0.1)
        self.dropout2 = tf.keras.layers.Dropout(0.1)
        self.dropout3 = tf.keras.layers.Dropout(0.1)

    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):
        attn1 = self.mha1(x, x, x, look_ahead_mask)
        attn1 = self.dropout1(attn1, training=training)
        out1 = self.layernorm1(x + attn1)
        attn2 = self.mha2(enc_output, enc_output, out1, padding_mask)
        attn2 = self.dropout2(attn2, training=training)
        out2 = self.layernorm2(out1 + attn2)
        ffn_output = self.ffn(out2)
        ffn_output = self.dropout3(ffn_output, training=training)
        out3 = self.layernorm3(out2 + ffn_output)
        return out3

class Transformer(tf.keras.Model):
    def _init_(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, max_length):
        super(Transformer, self)._init_()
        self.encoder_embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)
        self.decoder_embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)
        self.encoder_layers = [EncoderLayer(d_model, num_heads, dff) for _ in range(num_layers)]
        self.decoder_layers = [DecoderLayer(d_model, num_heads, dff) for _ in range(num_layers)]
        self.dropout = tf.keras.layers.Dropout(0.1)
        self.final_layer = tf.keras.layers.Dense(target_vocab_size)
        self.max_length = max_length

    def call(self, inputs, training=False):
        inp, tar = inputs
        enc_padding_mask = create_padding_mask(inp)
        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])
        dec_padding_mask = create_padding_mask(inp)

        enc_output = self.encoder_embedding(inp)
        for encoder_layer in self.encoder_layers:
            enc_output = encoder_layer(enc_output, training=training, mask=enc_padding_mask)

        dec_output = self.decoder_embedding(tar)
        for decoder_layer in self.decoder_layers:
            dec_output = decoder_layer(dec_output, enc_output, training=training, 
                                     look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)

        dec_output = self.dropout(dec_output, training=training)
        return self.final_layer(dec_output)

num_layers = 4
d_model = 128
num_heads = 8
dff = 512
input_vocab_size = len(en_tokenizer.word_index) + 1
target_vocab_size = len(ar_tokenizer.word_index) + 1
max_length = 50

transformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, max_length)
print("تم إنشاء موديل Transformer بنجاح!")
transformer.summary()



import tensorflow as tf

def loss_function(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    loss_ = tf.keras.losses.SparseCategoricalCrossentropy(
        from_logits=True, reduction='none')(real, pred)
    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask
    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)

class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def _init_(self, d_model, warmup_steps=4000):
        super(CustomSchedule, self)._init_()
        self.d_model = tf.cast(d_model, tf.float32)
        self.warmup_steps = tf.cast(warmup_steps, tf.float32)

    def _call_(self, step):
        step = tf.cast(step, tf.float32)
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)
        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

    def get_config(self):
        return {
            "d_model": self.d_model.numpy(),
            "warmup_steps": self.warmup_steps.numpy()
        }

learning_rate = CustomSchedule(d_model=128)
optimizer = tf.keras.optimizers.Adam(
    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)

print("تم إعداد التدريب بنجاح!")




import tensorflow as tf

@tf.function
def train_step(inp, tar):
    tar_inp = tar[:, :-1]
    tar_real = tar[:, 1:]
    
    with tf.GradientTape() as tape:
        predictions = transformer([inp, tar_inp], training=True)
        loss = loss_function(tar_real, predictions)
    
    gradients = tape.gradient(loss, transformer.trainable_variables)
    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))
    return loss

def train_model(transformer, en_padded, ar_padded, epochs=8, batch_size=32):
    print("بدء التدريب...")
    dataset = tf.data.Dataset.from_tensor_slices((en_padded, ar_padded))
    dataset = dataset.shuffle(buffer_size=len(en_padded)).batch(batch_size)
    
    for epoch in range(epochs):
        total_loss = 0
        for (batch, (inp, tar)) in enumerate(dataset):
            loss = train_step(inp, tar)
            total_loss += loss
            
            if batch % 100 == 0:
                print(f'Epoch {epoch+1} Batch {batch} Loss {loss.numpy():.4f}')
        
        avg_loss = total_loss / (batch + 1)
        print(f'Epoch {epoch+1} Loss {avg_loss:.4f}')
    
    print("تم حفظ النموذج في ./translation_model.keras")
    transformer.save('translation_model.keras')  # حفظ النموذج بصيغة .keras

train_model(transformer, en_padded, ar_padded, epochs=8, batch_size=32)


import tensorflow as tf

def loss_function(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    loss_ = tf.keras.losses.SparseCategoricalCrossentropy(
        from_logits=True, reduction='none')(real, pred)
    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask
    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)

class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def _init_(self, d_model, warmup_steps=4000):
        super(CustomSchedule, self)._init_()
        self.d_model = tf.cast(d_model, tf.float32)
        self.warmup_steps = tf.cast(warmup_steps, tf.float32)

    def _call_(self, step):
        step = tf.cast(step, tf.float32)
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)
        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

    def get_config(self):
        return {
            "d_model": self.d_model.numpy(),
            "warmup_steps": self.warmup_steps.numpy()
        }

learning_rate = CustomSchedule(d_model=128)
optimizer = tf.keras.optimizers.Adam(
    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)

print("تم إعداد التدريب بنجاح!")


transformer.compile(optimizer=optimizer, loss=loss_function)
print("تم ربط المُحسّن بالنموذج بنجاح!")

transformer.save('translation_model.keras')
print("تم حفظ النموذج بنجاح في ./translation_model.keras")


import tensorflow as tf
import numpy as np

def translate(sentence, transformer, en_tokenizer, ar_tokenizer, max_length=50):
    inputs = [en_tokenizer.word_index.get(word, 0) for word in sentence.lower().split()]
    inputs = [2] + inputs + [3]  # إضافة <start> (2) و<end> (3)
    inputs = inputs[:max_length] + [0] * (max_length - len(inputs))  # padding
    inputs = tf.convert_to_tensor([inputs], dtype=tf.int32)

    output = [2]  # <start>
    output = tf.convert_to_tensor([output], dtype=tf.int32)

    for _ in range(max_length):
        predictions = transformer([inputs, output], training=False)
        predictions = predictions[:, -1, :]  # آخر توكن متوقع
        predicted_id = tf.argmax(predictions, axis=-1)
        predicted_id = tf.cast(predicted_id, tf.int32)

        output = tf.concat([output, [predicted_id]], axis=-1)

        if predicted_id.numpy()[0] == 3:  # <end>
            break

    ar_sentence = [ar_tokenizer.index_word.get(idx, '') for idx in output.numpy()[0]]
    # إزالة <start> و<end> وأي كلمات فارغة
    ar_sentence = ' '.join([word for word in ar_sentence if word and word not in ['<start>', '<end>']])
    return ar_sentence

sentences = ["Thank you", "I am happy", "Where are you"]
for sentence in sentences:
    translated = translate(sentence, transformer, en_tokenizer, ar_tokenizer)
    print(f"English: {sentence}")
    print(f"Arabic: {translated}")