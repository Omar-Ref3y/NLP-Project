import os
import numpy as np
import pandas as pd
import pickle
from tqdm import tqdm
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

# 1. Load and preprocess captions
captions = pd.read_csv('/kaggle/input/flickr8k/captions.txt')
captions.columns = ['image', 'caption']
captions['caption'] = captions['caption'].apply(lambda x: 'startseq ' + x + ' endseq')

# 2. Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(captions['caption'])
vocab_size = len(tokenizer.word_index) + 1
max_length = max(len(c.split()) for c in captions['caption'])

# 3. Extract image features using VGG16
vgg_model = VGG16(weights='imagenet', include_top=False, pooling='avg')
image_dir = '/kaggle/input/flickr8k/Images'
image_features = {}

for img_name in tqdm(captions['image'].unique(), desc="Extracting image features"):
    img_path = os.path.join(image_dir, img_name)
    if os.path.exists(img_path):
        img = load_img(img_path, target_size=(224, 224))
        img = img_to_array(img)
        img = preprocess_input(img)
        img = np.expand_dims(img, axis=0)
        feature = vgg_model.predict(img, verbose=0)
        image_features[img_name] = feature.flatten()

np.save("image_features_full.npy", image_features)

# 4. Define the CNN + LSTM model
embedding_dim = 256

inputs1 = Input(shape=(4096,))
fe1 = Dropout(0.5)(inputs1)
fe2 = Dense(256, activation='relu')(fe1)

inputs2 = Input(shape=(max_length,))
se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)
se2 = Dropout(0.5)(se1)
se3 = LSTM(256)(se2)

decoder1 = add([fe2, se3])
decoder2 = Dense(256, activation='relu')(decoder1)
outputs = Dense(vocab_size, activation='softmax')(decoder2)

caption_model = Model(inputs=[inputs1, inputs2], outputs=outputs)
caption_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')
caption_model.summary()

# 5. Prepare training data
sequences = tokenizer.texts_to_sequences(captions['caption'])
X1, X2, y = [], [], []

for i in tqdm(range(len(captions))):
    img = captions['image'][i]
    seq = sequences[i]
    for j in range(1, len(seq)):
        in_seq, out_seq = seq[:j], seq[j]
        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
        out_seq = np.array(out_seq)
        X1.append(image_features[img])
        X2.append(in_seq)
        y.append(out_seq)

X1 = np.array(X1)
X2 = np.array(X2)
y = np.array(y, dtype='int32')

# 6. Train the model
caption_model.fit([X1, X2], y, epochs=10, batch_size=256, verbose=1)

# 7. Save model and tokenizer
caption_model.save("caption_model_full.h5")
with open("tokenizer_full.pkl", "wb") as f:
    pickle.dump(tokenizer, f)

# 8. Beam Search decoding
def generate_desc_beam(model, tokenizer, photo, max_length, beam_index=3):
    start = [tokenizer.word_index['startseq']]
    start_word = [[start, 0.0]]

    while len(start_word[0][0]) < max_length:
        temp = []
        for s in start_word:
            sequence = pad_sequences([s[0]], maxlen=max_length)
            preds = model.predict([photo, sequence], verbose=0)
            word_preds = np.argsort(preds[0])[-beam_index:]

            for w in word_preds:
                next_seq = s[0] + [w]
                prob = s[1] + np.log(preds[0][w] + 1e-10)
                temp.append([next_seq, prob])

        start_word = sorted(temp, reverse=False, key=lambda l: l[1])
        start_word = start_word[-beam_index:]

    final_seq = start_word[-1][0]
    final_caption = [tokenizer.index_word[i] for i in final_seq if i in tokenizer.index_word]
    final_caption = final_caption[1:-1]  # remove startseq and endseq
    return ' '.join(final_caption)