import os
import numpy as np
import pandas as pd
import tensorflow as tf
import logging
logging.getLogger('tensorflow').setLevel(logging.ERROR)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, Concatenate, Layer, Reshape
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.applications.efficientnet import preprocess_input
from tensorflow.keras.callbacks import EarlyStopping
import pickle
import kagglehub
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.regularizers import l2

# Custom Bahdanau Attention Layer
class BahdanauAttention(Layer):
    def __init__(self, units):
        super(BahdanauAttention, self).__init__()
        self.W1 = Dense(units, kernel_regularizer=l2(0.01))
        self.W2 = Dense(units, kernel_regularizer=l2(0.01))
        self.V = Dense(1)

    def call(self, features, hidden):
        hidden_with_time_axis = Reshape((1, -1))(hidden)
        score = self.V(tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))
        attention_weights = tf.nn.softmax(score, axis=1)
        context_vector = attention_weights * features
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector

# Download dataset path
path = kagglehub.dataset_download("adityajn105/flickr8k")
print("Path to dataset files:", path)

# Define paths
images_dir = os.path.join(path, 'Images')
captions_file = os.path.join(path, 'captions.txt')

# Create directory for saving features if it doesn't exist
os.makedirs('/kaggle/working/model_data', exist_ok=True)

# Step 1: Feature Extraction with EfficientNetB0
print("Starting feature extraction with EfficientNetB0...")
features_path = '/kaggle/working/model_data/efficientnetb0_features.pickle'

# Check if features already exist
if not os.path.exists(features_path):
    # Load EfficientNetB0 model
    try:
        base_model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')
        model = Model(inputs=base_model.input, outputs=base_model.output)
    except Exception as e:
        raise RuntimeError(f"Failed to load EfficientNetB0 model: {e}")

    # Data augmentation
    datagen = ImageDataGenerator(
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        horizontal_flip=True,
        brightness_range=[0.8, 1.2]
    )

    # Function to extract features with augmentation
    def extract_features(image_path, model, datagen):
        try:
            img = load_img(image_path, target_size=(224, 224))
            img = img_to_array(img)
            img = datagen.random_transform(img)
            img = np.expand_dims(img, axis=0)
            img = preprocess_input(img)
            features = model.predict(img, verbose=0)
            return features
        except Exception as e:
            print(f"Error processing image {image_path}: {e}")
            return None

    # Extract features for all images
    features = {}
    image_files = [f for f in os.listdir(images_dir) if f.endswith('.jpg')]
    if not image_files:
        raise FileNotFoundError("No .jpg images found in the images directory.")

    for i, img_name in enumerate(image_files):
        img_path = os.path.join(images_dir, img_name)
        feature = extract_features(img_path, model, datagen)
        if feature is not None:
            features[img_name] = feature
        if (i + 1) % 100 == 0:
            print(f"Processed {i + 1}/{len(image_files)} images")

    # Verify that features were extracted
    if not features:
        raise ValueError("No features were extracted. Please check the image processing step.")

    # Save features
    try:
        with open(features_path, 'wb') as handle:
            pickle.dump(features, handle, protocol=pickle.HIGHEST_PROTOCOL)
        print(f"Saved features to: {features_path}")
    except Exception as e:
        raise RuntimeError(f"Failed to save features to {features_path}: {e}")
else:
    print(f"Features already exist at {features_path}. Skipping feature extraction.")

# Load saved features
if os.path.exists(features_path):
    with open(features_path, 'rb') as handle:
        features = pickle.load(handle)
    print(f"Loaded features for {len(features)} images from: {features_path}")
else:
    raise FileNotFoundError(f"Features file not found at {features_path}. Feature extraction may have failed.")

# Load captions
df = pd.read_csv(captions_file)

# Group captions by image
image_captions = {}
for index, row in df.iterrows():
    image_name = row['image']
    caption = row['caption']
    if image_name not in image_captions:
        image_captions[image_name] = []
    image_captions[image_name].append(caption)

# Clean captions
def clean_caption(caption):
    caption = caption.lower()
    caption = ''.join(c for c in caption if c.isalnum() or c.isspace())
    caption = ' '.join(caption.split())
    caption = 'startseq ' + caption + ' endseq'
    return caption

for img, captions in image_captions.items():
    image_captions[img] = [clean_caption(caption) for caption in captions]

# Create a list of all captions
all_captions = []
for img, captions in image_captions.items():
    all_captions.extend(captions)

# Create/load tokenizer
tokenizer_path = 'tokenizer.pickle'
if os.path.exists(tokenizer_path):
    with open(tokenizer_path, 'rb') as handle:
        tokenizer = pickle.load(handle)
else:
    tokenizer = Tokenizer(oov_token='<OOV>')
    tokenizer.fit_on_texts(all_captions)
    with open(tokenizer_path, 'wb') as handle:
        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

vocab_size = len(tokenizer.word_index) + 1
print(f"Vocabulary Size: {vocab_size}")

# Find maximum caption length
max_length = max(len(caption.split()) for caption in all_captions)
print(f"Maximum Caption Length: {max_length}")

# Data generator
def data_generator(descriptions, photos, tokenizer, max_length, vocab_size, batch_size):
    X1, X2, y = [], [], []
    while True:
        keys = list(descriptions.keys())
        np.random.shuffle(keys)
        for key in keys:
            if key not in photos:
                continue
            photo = photos[key][0]
            for caption in descriptions[key]:
                seq = tokenizer.texts_to_sequences([caption])[0]
                for i in range(1, len(seq)):
                    in_seq, out_seq = seq[:i], seq[i]
                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
                    out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=vocab_size)[0]
                    X1.append(photo)
                    X2.append(in_seq)
                    y.append(out_seq)
                if len(X1) >= batch_size:
                    yield (
                        tf.convert_to_tensor(X1, dtype=tf.float32),
                        tf.convert_to_tensor(X2, dtype=tf.int32)
                    ), tf.convert_to_tensor(y, dtype=tf.float32)
                    X1, X2, y = [], [], []
            if len(X1) > 0:
                yield (
                    tf.convert_to_tensor(X1, dtype=tf.float32),
                        tf.convert_to_tensor(X2, dtype=tf.int32)
                ), tf.convert_to_tensor(y, dtype=tf.float32)
                X1, X2, y = [], [], []

# Create tf.data.Dataset
def create_dataset(descriptions, photos, tokenizer, max_length, vocab_size, batch_size):
    dataset = tf.data.Dataset.from_generator(
        lambda: data_generator(descriptions, photos, tokenizer, max_length, vocab_size, batch_size),
        output_signature=(
            (
                tf.TensorSpec(shape=(None, 1280), dtype=tf.float32),
                tf.TensorSpec(shape=(None, max_length), dtype=tf.int32)
            ),
            tf.TensorSpec(shape=(None, vocab_size), dtype=tf.float32)
        )
    )
    return dataset.cache().prefetch(tf.data.AUTOTUNE)

# Split data
train_images = list(features.keys())[:6000]
test_images = list(features.keys())[6000:]
train_descriptions = {k: image_captions[k] for k in train_images if k in image_captions}
test_descriptions = {k: image_captions[k] for k in test_images if k in image_captions}
print(f"Training images: {len(train_descriptions)}")
print(f"Testing images: {len(test_descriptions)}")

# Calculate steps_per_epoch and validation_steps before create_model
batch_size = 64
steps_per_epoch = (sum(len(captions) for captions in train_descriptions.values()) // batch_size) + 1
validation_steps = (sum(len(captions) for captions in test_descriptions.values()) // batch_size) + 1

# Create model with attention
def create_model(vocab_size, max_length):
    inputs1 = Input(shape=(1280,))
    fe1 = Dropout(0.5)(inputs1)
    fe2 = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(fe1)
    fe3 = Reshape((1, 512))(fe2)

    inputs2 = Input(shape=(max_length,))
    se1 = Embedding(vocab_size, 512, mask_zero=True)(inputs2)
    se2 = Dropout(0.5)(se1)
    se3 = LSTM(512, return_sequences=True, use_cudnn=False)(se2)
    se4 = LSTM(512, return_sequences=False, use_cudnn=False)(se3)

    attention = BahdanauAttention(512)
    context_vector = attention(fe3, se4)

    concat = Concatenate()([context_vector, se4])
    decoder1 = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(concat)
    outputs = Dense(vocab_size, activation='softmax')(decoder1)

    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    
    # Learning rate scheduler
    initial_learning_rate = 0.0005
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate,
        decay_steps=steps_per_epoch * 5,
        decay_rate=0.95,
        staircase=True)
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
    
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

print("Creating model...")
model = create_model(vocab_size, max_length)
model.summary()

# Create datasets
train_dataset = create_dataset(train_descriptions, features, tokenizer, max_length, vocab_size, batch_size)
test_dataset = create_dataset(test_descriptions, features, tokenizer, max_length, vocab_size, batch_size)

# Early stopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=20,
    restore_best_weights=True
)

# Train model
print("Training model...")
history = model.fit(
    train_dataset,
    epochs=80,
    steps_per_epoch=steps_per_epoch,
    validation_data=test_dataset,
    validation_steps=validation_steps,
    callbacks=[early_stopping]
)

# Save model
model.save('image_captioning_model.h5')

# Generate captions with Beam Search
def generate_caption(model, tokenizer, photo, max_length, beam_width=5):
    photo = np.squeeze(photo)
    photo = np.expand_dims(photo, axis=0)
    start = [tokenizer.word_index['startseq']]
    sequences = [[start, 0.0]]

    for _ in range(max_length):
        all_candidates = []
        for seq, score in sequences:
            padded_seq = pad_sequences([seq], maxlen=max_length)
            yhat = model.predict([photo, padded_seq], verbose=0)
            top_words = np.argsort(yhat[0])[-beam_width:]
            for word_idx in top_words:
                new_seq = seq + [word_idx]
                new_score = score - np.log(yhat[0, word_idx] + 1e-8)
                all_candidates.append([new_seq, new_score])
        sequences = sorted(all_candidates, key=lambda x: x[1])[:beam_width]
        if sequences[0][0][-1] == tokenizer.word_index.get('endseq', 0):
            break

    best_seq = sequences[0][0]
    caption = ' '.join([tokenizer.index_word.get(idx, '') for idx in best_seq if idx in tokenizer.index_word])
    return caption.replace('startseq', '').replace('endseq', '').strip()

# Test model with image display
def test_model(model, images_dir, test_images, features, tokenizer, max_length):
    test_samples = test_images[:5]
    for img_name in test_samples:
        img_path = os.path.join(images_dir, img_name)
        img = load_img(img_path, target_size=(224, 224))
        plt.figure(figsize=(8, 8))
        plt.imshow(img)
        plt.axis('off')

        photo = features[img_name]
        caption = generate_caption(model, tokenizer, photo, max_length)

        actual_captions = '\n'.join(image_captions[img_name])
        display_text = f"Generated Caption: {caption}\n\nActual Captions:\n{actual_captions}"

        plt.title(display_text, fontsize=12, wrap=True)
        plt.tight_layout()
        plt.show()

        print(f"Image: {img_name}")
        print(f"Generated Caption: {caption}")
        print(f"Actual Captions: {image_captions[img_name]}")
        print("-" * 50)

print("Testing model...")
test_model(model, images_dir, test_images, features, tokenizer, max_length)

# Plot training history
plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Training History')
plt.xlabel('Epochs')
plt.ylabel('Metrics')
plt.legend()
plt.savefig('training_history.png')
plt.show()

print("Image captioning model training complete!")
